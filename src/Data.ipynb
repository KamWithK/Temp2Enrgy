{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project overview\n",
    "Energy is used on a daily basis for phones, computers, washing machines, heaters and a vast array of appliances.\n",
    "Our dependence on electricity makes it critical to accurately predict how much will need to generate on any given day.\n",
    "Hence, our project aims to correlate and model temperature's effect on energy demand.\n",
    "We will begin by importing and cleaning our two datasets, before analysing the data integrity and creating visualisations to intuitively highlight the impact several variables have on Australia's net energy useage.\n",
    "These visualisations aim to show not only energy and temperature, but yearly trends, seasonal shifts and other potential anomalies.\n",
    "After this we create a basic timeseries model, to evaluate precisely how well we're able to measure energy demand on a day-to-day basis.\n",
    "This report will conclude with a summary of our results (including the significance of different independent and extraneous variables) and a series of suggestions on how our work can be improved going forwards.\n",
    "\n",
    "To help achieve these results we will use a range of technical and statistical tools.\n",
    "We heavily rely on Python and its associated libraries (largely Pandas for data cleaning and management, Numpy for numerical calculations, MatPlotLib for graphs and Scikit learn for machine learning modelling).\n",
    "\n",
    "# Supplied data\n",
    "Our weather data is collected from the Bureau of Meteorology's (BOM) Automatic Weather Stations and energy data from The Australian Energy Market Operator (AEMO).\n",
    "The data comes in the form of a series of CSVs containing measurements every 30 minutes.\n",
    "The weather data contains precipitation (mm), temperature (°C), relative humidity (%), wind speed (km/h), wind direction (° true), maximum windgust speed (km/h), pressure (hPa) and whether manual/automatic measurements were taken.\n",
    "It is quite common for there to be multiple features (columns) with nearly identical data (i.e. different forms of temperature or pressure).\n",
    "Energy data provides total demand and a RRP (energy price).\n",
    "\n",
    "The large portion of this data will be processed to elliminate any present trends, biases or inconsistancies. Yet, not all the provided data from BOM and AEMO are relavent (so many columns are removed).\n",
    "\n",
    "# Data Cleaning\n",
    "## Import the data\n",
    "When importing data, we first need to find a list of all the data files/their paths.\n",
    "We can then merge the separate spreadsheets into two dataframes (one for temperature and one for energy).\n",
    "The process of finding the file paths, loading the seperate CSV files and then finally concatenating them together can be tedious, however this ultimately allows for easy access to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamron/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (12,14,18,20,22,24,26,28,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kamron/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (12,14,18,20,22,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kamron/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (12,14,18,20,28,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/kamron/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (12,16,22,24,26,28,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "energy_locations = os.listdir(\"../Data/Energy\")\n",
    "temperature_locations = os.listdir(\"../Data/Temperature\")\n",
    "\n",
    "energy_CSVs = [pd.read_csv(\"../Data/Energy/\" + location) for location in energy_locations]\n",
    "temperature_CSVs = [pd.read_csv(\"../Data/Temperature/\" + location) for location in temperature_locations if \"Data\" in location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data = pd.concat(energy_CSVs, ignore_index=True)\n",
    "temperature_data = pd.concat(temperature_CSVs, ignore_index=True)\n",
    "\n",
    "# Note some excess data is shaved off here for simplicity\n",
    "stations_data = pd.read_csv(\"../Data/Temperature/HM01X_StnDet_999999999743964.txt\", skiprows=5, header=None, usecols=range(1, 21), names=[\"StationNumber\", \"RainfulDistrict\", \"Station\", \"StationOpenDate\", \"StationCloseDate\", \"Latitude\", \"Longitude\", \"LocationMethod\", \"State\", \"StationHeight\", \"BarometerHeight\", \"WMO\", \"FirstYear\", \"LastYear\", \"CompletionPercent\", \"YQualityPercent\", \"NQualityPercent\", \"WQualityPercent\", \"SQualityPercent\", \"IQualityPercent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column standardising\n",
    "To ensure it is easy to find specific data features, all column names are standardised.\n",
    "We have chosen to use PascalCase here.\n",
    "\n",
    "As there are two sets of dates available here, we append a 1 to the end of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Region', 'Date', 'TotalDemand', 'RRP'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['Precipitation', 'AirTemperature', 'WetBulbTemperature',\n",
       "       'DewTemperature', 'RelativeHumidity', 'SeaPressure', 'StationPressure',\n",
       "       'AWSFlag', 'Date', 'Region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['StationNumber', 'RainfulDistrict', 'Station', 'Latitude', 'Longitude',\n",
       "       'LocationMethod', 'State', 'StationHeight', 'BarometerHeight', 'WMO',\n",
       "       'CompletionPercent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_data.columns\n",
    "temperature_data.columns\n",
    "stations_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.columns = [\"Region\", \"Date\", \"TotalDemand\", \"RRP\", \"PeriodType\"]\n",
    "temperature_data.columns = [\n",
    "    \"HM\", \"StationNumber\", \"Year1\", \"Month1\", \"Day1\", \"Hour1\", \"Minute1\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"Precipitation\", \"PrecipitationQuality\",\n",
    "    \"AirTemperature\", \"AirTemperatureQuality\", \"WetBulbTemperature\", \"WetBulbTemperatureQuality\", \"DewTemperature\", \"DewTemperatureQuality\", \"RelativeHumidity\",\n",
    "    \"RelativeHumidityQuality\", \"WindSpeed\", \"WindSpeedQuality\", \"WindDirection\", \"WindDirectionQuality\", \"WindgustSpeed\", \"WindgustSpeedQuality\", \"SeaPressure\",\n",
    "    \"SeaPressureQuality\", \"StationPressure\", \"StationPressureQuality\", \"AWSFlag\", \"#\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any easily found unnecessary data\n",
    "Our dataset contains a multitude of columns, and several have absolutely no information within them.\n",
    "As these feature columns often contain a variety of null types (sometimes Numpy's NaN, sometimes an empty string and sometimes a 0 for a non-integer or category column), we analyse the number of unique elements.\n",
    "\n",
    "Reducing the number of columns present additionally makes it easy to quickly look at the state of the data when moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_uniques(dataframe: pd.DataFrame, filter = []):\n",
    "    remove = [name for name, series in dataframe.items() if len(series.unique()) <= 2 and not name in filter]\n",
    "    dataframe.drop(remove, axis=1, inplace=True)\n",
    "    return remove\n",
    "\n",
    "print(\"Removed:\")\n",
    "remove_non_uniques(energy_data)\n",
    "remove_non_uniques(temperature_data)\n",
    "remove_non_uniques(stations_data, \"LocationMethod\")\n",
    "\n",
    "# Manually remove extra columns\n",
    "stations_data.drop(\"StationOpenDate\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Region', 'Date', 'TotalDemand', 'RRP'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['StationNumber', 'Year1', 'Month1', 'Day1', 'Hour1', 'Minute1', 'Year',\n",
       "       'Month', 'Day', 'Hour', 'Minute', 'Precipitation', 'AirTemperature',\n",
       "       'WetBulbTemperature', 'DewTemperature', 'RelativeHumidity', 'WindSpeed',\n",
       "       'WindDirection', 'WindgustSpeed', 'SeaPressure', 'StationPressure',\n",
       "       'AWSFlag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['StationNumber', 'RainfulDistrict', 'Station', 'Latitude', 'Longitude',\n",
       "       'LocationMethod', 'State', 'StationHeight', 'BarometerHeight', 'WMO',\n",
       "       'CompletionPercent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_data.columns\n",
    "temperature_data.columns\n",
    "stations_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with multiple dates\n",
    "The provided data contains two sets of date values.\n",
    "The provided notes state that first dates are local and the second are local standard.\n",
    "Whilst generally identical, local time incorporates day light saving (shifting the clock back/foward by an hour).\n",
    "For simplicity, local standard time is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra dates\n",
    "temperature_data.drop([\"Year1\", \"Month1\", \"Day1\", \"Hour1\", \"Minute1\"], axis=1, inplace=True)\n",
    "\n",
    "# Reformat dates into Pandas' datatime64 objects\n",
    "# Replacing old format\n",
    "temperature_data[\"Date\"] = pd.to_datetime(temperature_data[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
    "energy_data[\"Date\"] = pd.to_datetime(energy_data[\"Date\"])\n",
    "\n",
    "temperature_data.drop([\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicated rows\n",
    "On top of finding and removing useless columns, we can also drop useless rows.\n",
    "This is far more straightforward as the provided Pandas functions work flawlessly with almost no manual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.drop_duplicates(inplace=True)\n",
    "temperature_data.drop_duplicates(inplace=True)\n",
    "stations_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting datatypes\n",
    "The next step is to convert the data types. \n",
    "To ensure that we can plot and model our data, we use specific Pandas/Python datatypes.\n",
    "The use of specific datatypes like DateTime and Category are crucial in providing a simple but flexible API.\n",
    "This will latter allow indexing through dates (so ```loc``` with multiple conditional statements aren't  repeatidly used)!\n",
    "\n",
    "Although Pandas includes functions for parsing data, these are of little use because of the special scenareos present here.\n",
    "These include spaces before and after numbers and hashtags in a random location.\n",
    "To easily overcome these issues a function is created which finds all columns with generic objects, and performs some operation on each entry.\n",
    "This works specifically because Pandas does not set a specific datatype for strings.\n",
    "\n",
    "For AWS Flag, we assume that null values are recorded manually, as this is the worst case scenareo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_object_columns(lambda_function):\n",
    "    string_columns = temperature_data.select_dtypes(\"object\").columns\n",
    "    temperature_data[string_columns] = temperature_data[string_columns].apply(lambda_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_object_columns(lambda column: column.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data[\"AWSFlag\"] = temperature_data[\"AWSFlag\"].replace(\"\", 0).astype(\"category\")\n",
    "temperature_data[\"AWSFlag\"].fillna(0, inplace=True)\n",
    "temperature_data[\"RelativeHumidity\"] = temperature_data[\"RelativeHumidity\"].replace(\"###\", np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_object_columns(lambda column: pd.to_numeric(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StationNumber                  int64\n",
       "Precipitation                float64\n",
       "AirTemperature               float64\n",
       "WetBulbTemperature           float64\n",
       "DewTemperature               float64\n",
       "RelativeHumidity             float64\n",
       "WindSpeed                    float64\n",
       "WindDirection                float64\n",
       "WindgustSpeed                float64\n",
       "SeaPressure                  float64\n",
       "StationPressure              float64\n",
       "AWSFlag                     category\n",
       "Date                  datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding missing data\n",
    "Through analysing where and how our data is missing, we are able conclude what is worth keeping and what should be removed/interpolated.\n",
    "We of course can only do this for the temperature data, as there is no missing energy data.\n",
    "\n",
    "As we can tell, a large chunk of wind data is missing and so these columns must be removed.\n",
    "\n",
    "If we graph out each time series (in the graphing notebook), we notice that the missing data for the other columns (like wet bulb temperature) is decently randomly distributed.\n",
    "This means that it should be relatively safe to interpolate for the missing values (i.e. reason using linear algebra what the value should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_counts(dataframe: pd.DataFrame):\n",
    "    return dataframe.isnull().mean()[dataframe.isnull().mean() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Precipitation         0.229916\n",
       "AirTemperature        0.444437\n",
       "WetBulbTemperature    0.011324\n",
       "DewTemperature        0.375311\n",
       "RelativeHumidity      0.375312\n",
       "WindSpeed             0.532966\n",
       "WindDirection         0.432305\n",
       "WindgustSpeed         0.403183\n",
       "SeaPressure           0.137730\n",
       "StationPressure       0.011135\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_null_counts(energy_data)\n",
    "get_null_counts(temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle([energy_data, temperature_data, stations_data], \"../Data/CleanedData.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data.drop([\"WindSpeed\", \"WindgustSpeed\", \"WindDirection\"], axis=1, inplace=True)\n",
    "\n",
    "# Note that using inplace currently throws an error\n",
    "# So interpolated columns must be manually overridden\n",
    "missing_columns = list(get_null_counts(temperature_data).keys())\n",
    "temperature_data[missing_columns] = temperature_data[missing_columns].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data\n",
    "Energy and temperature data are recorded in specific regions, however they use different keys/criteria (so we must align them ourselves).\n",
    "This is the intermediary stage before the two datasets can be joined together.\n",
    "\n",
    "More than one weather station existed in Victoria (for a short time) and here both stations data are just added together.\n",
    "*Although this may lead to a slightly biased dataset (if there ends up being slightly more data for Victoria than other states), using seperate models for seperate states will eliminate this issue*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VIC1', 'SA1', 'TAS1', 'QLD1', 'NSW1'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([94029, 86071, 66062, 40913, 86338, 23090])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_data[\"Region\"].unique()\n",
    "temperature_data[\"StationNumber\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_remove_number_map = {\"SA1\": \"SA\", \"QLD1\": \"QLD\", \"NSW1\": \"NSW\", \"VIC1\": \"VIC\", \"TAS1\": \"TAS\"}\n",
    "station_to_region_map = {23090: \"SA\", 40913: \"QLD\", 66062: \"NSW\", 86071: \"VIC\", 94029: \"TAS\", 86338: \"VIC\"}\n",
    "\n",
    "temperature_data[\"Region\"] = temperature_data[\"StationNumber\"].map(station_to_region_map)\n",
    "energy_data[\"Region\"] = energy_data[\"Region\"].map(region_remove_number_map)\n",
    "\n",
    "temperature_data.drop(\"StationNumber\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining energy and temperature data\n",
    "To be able to create a model which predicts energy demand using temperature data we need to align our temperature and energy measurements together (i.e. by date).\n",
    "This is accomplished through closest value merges.\n",
    "These merges work with sorted data, so we begin by sorting our data by date.\n",
    "We can then use the ```merge_asof``` function (where we specify that our data is grouped ```by``` region) to merge together the two datasets (where the closest entries get combined).\n",
    "As our data is nearly always recorded in 30 minute intervals, we only merge two rows if there individual dates differ by less than 30 minutes.\n",
    "\n",
    "Any unpaired rows result in null values, and since we have a small number of them we can tell that our merge happened successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.sort_values(by=\"Date\", inplace=True)\n",
    "temperature_data.sort_values(by=\"Date\", inplace=True)\n",
    "\n",
    "data = pd.merge_asof(energy_data, temperature_data, on=\"Date\", by=\"Region\", tolerance=pd.Timedelta(\"30 min\"))\n",
    "data.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Precipitation         0.001634\n",
       "AirTemperature        0.001634\n",
       "WetBulbTemperature    0.001634\n",
       "DewTemperature        0.001634\n",
       "RelativeHumidity      0.001634\n",
       "SeaPressure           0.001634\n",
       "StationPressure       0.001634\n",
       "AWSFlag               0.001634\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_null_counts(data)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>TotalDemand</th>\n",
       "      <th>RRP</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>AirTemperature</th>\n",
       "      <th>WetBulbTemperature</th>\n",
       "      <th>DewTemperature</th>\n",
       "      <th>RelativeHumidity</th>\n",
       "      <th>SeaPressure</th>\n",
       "      <th>StationPressure</th>\n",
       "      <th>AWSFlag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:30:00</th>\n",
       "      <td>NSW</td>\n",
       "      <td>6763.57000</td>\n",
       "      <td>15.64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.413604</td>\n",
       "      <td>14.1</td>\n",
       "      <td>10.539772</td>\n",
       "      <td>77.829696</td>\n",
       "      <td>1011.100000</td>\n",
       "      <td>1006.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:30:00</th>\n",
       "      <td>SA</td>\n",
       "      <td>1328.68667</td>\n",
       "      <td>38.54</td>\n",
       "      <td>1.239962</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1018.700000</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:30:00</th>\n",
       "      <td>QLD</td>\n",
       "      <td>3905.56833</td>\n",
       "      <td>39.40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.686886</td>\n",
       "      <td>19.2</td>\n",
       "      <td>17.185578</td>\n",
       "      <td>80.606576</td>\n",
       "      <td>1007.200000</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:30:00</th>\n",
       "      <td>VIC</td>\n",
       "      <td>4419.03667</td>\n",
       "      <td>9.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>10.5</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 01:00:00</th>\n",
       "      <td>SA</td>\n",
       "      <td>1375.14833</td>\n",
       "      <td>38.54</td>\n",
       "      <td>1.239886</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>1012.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>QLD</td>\n",
       "      <td>6218.39000</td>\n",
       "      <td>52.15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.530150</td>\n",
       "      <td>21.6</td>\n",
       "      <td>11.839619</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1019.643632</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>VIC</td>\n",
       "      <td>4372.27000</td>\n",
       "      <td>85.23</td>\n",
       "      <td>1.313453</td>\n",
       "      <td>13.143757</td>\n",
       "      <td>12.3</td>\n",
       "      <td>6.642222</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1014.900000</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>SA</td>\n",
       "      <td>1474.11000</td>\n",
       "      <td>85.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>12.3</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1019.900000</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>NSW</td>\n",
       "      <td>7318.64000</td>\n",
       "      <td>50.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.680430</td>\n",
       "      <td>15.7</td>\n",
       "      <td>17.139098</td>\n",
       "      <td>80.412908</td>\n",
       "      <td>1007.799390</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>TAS</td>\n",
       "      <td>1006.70000</td>\n",
       "      <td>93.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.756677</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7.225640</td>\n",
       "      <td>64.878548</td>\n",
       "      <td>1016.925601</td>\n",
       "      <td>1004.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1656254 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Region  TotalDemand    RRP  Precipitation  AirTemperature  \\\n",
       "Date                                                                            \n",
       "2000-01-01 00:30:00    NSW   6763.57000  15.64       0.000000       14.413604   \n",
       "2000-01-01 00:30:00     SA   1328.68667  38.54       1.239962       13.200000   \n",
       "2000-01-01 00:30:00    QLD   3905.56833  39.40       0.000000       20.686886   \n",
       "2000-01-01 00:30:00    VIC   4419.03667   9.97       0.000000       13.700000   \n",
       "2000-01-01 01:00:00     SA   1375.14833  38.54       1.239886       13.400000   \n",
       "...                    ...          ...    ...            ...             ...   \n",
       "2020-01-01 00:00:00    QLD   6218.39000  52.15       0.000000       18.530150   \n",
       "2020-01-01 00:00:00    VIC   4372.27000  85.23       1.313453       13.143757   \n",
       "2020-01-01 00:00:00     SA   1474.11000  85.95       0.000000       17.000000   \n",
       "2020-01-01 00:00:00    NSW   7318.64000  50.01       0.000000       20.680430   \n",
       "2020-01-01 00:00:00    TAS   1006.70000  93.88       0.000000       13.756677   \n",
       "\n",
       "                     WetBulbTemperature  DewTemperature  RelativeHumidity  \\\n",
       "Date                                                                        \n",
       "2000-01-01 00:30:00                14.1       10.539772         77.829696   \n",
       "2000-01-01 00:30:00                10.1        6.800000         65.000000   \n",
       "2000-01-01 00:30:00                19.2       17.185578         80.606576   \n",
       "2000-01-01 00:30:00                10.5        7.200000         65.000000   \n",
       "2000-01-01 01:00:00                10.1        6.500000         63.000000   \n",
       "...                                 ...             ...               ...   \n",
       "2020-01-01 00:00:00                21.6       11.839619         65.000000   \n",
       "2020-01-01 00:00:00                12.3        6.642222         65.000000   \n",
       "2020-01-01 00:00:00                12.3        8.700000         61.000000   \n",
       "2020-01-01 00:00:00                15.7       17.139098         80.412908   \n",
       "2020-01-01 00:00:00                 9.5        7.225640         64.878548   \n",
       "\n",
       "                     SeaPressure  StationPressure AWSFlag  \n",
       "Date                                                       \n",
       "2000-01-01 00:30:00  1011.100000           1006.4       0  \n",
       "2000-01-01 00:30:00  1018.700000           1012.6       0  \n",
       "2000-01-01 00:30:00  1007.200000           1006.2       0  \n",
       "2000-01-01 00:30:00  1017.000000           1013.1       0  \n",
       "2000-01-01 01:00:00  1018.500000           1012.4       0  \n",
       "...                          ...              ...     ...  \n",
       "2020-01-01 00:00:00  1019.643632           1013.8       0  \n",
       "2020-01-01 00:00:00  1014.900000           1014.0       0  \n",
       "2020-01-01 00:00:00  1019.900000           1010.6       0  \n",
       "2020-01-01 00:00:00  1007.799390           1008.8       0  \n",
       "2020-01-01 00:00:00  1016.925601           1004.6       0  \n",
       "\n",
       "[1656254 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(data, \"../Data/Data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
